{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad504558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<requests_oauthlib.oauth1_session.OAuth1Session object at 0x7fd792faa430>\n",
      "投稿準備完了\n",
      "スニダンのページの取得完了\n",
      "画像の取得完了\n",
      "ここから詳細ページ\n",
      "投稿済みです\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"/Users/kuramochiosuke/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\")\n",
    "from requests_oauthlib import OAuth1Session\n",
    "sys.path.append(\"/Users/kuramochiosuke/.pyenv/versions/3.10.4/lib/python3.10/site-packages\")\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import requests\n",
    "import schedule\n",
    "import time\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "\n",
    "# .envファイルを探して読み込み\n",
    "env_file = find_dotenv()\n",
    "load_dotenv(env_file)  \n",
    "\n",
    "CONSUMER_KEY = os.environ.get('CONSUMER_KEY')\n",
    "CONSUMER_SECRET = os.environ.get('CONSUMER_SECRET')\n",
    "ACCESS_KEY = os.environ.get('ACCESS_KEY')\n",
    "ACCESS_KEY_SECRET = os.environ.get('ACCESS_KEY_SECRET')\n",
    "\n",
    "# Twitterの認証\n",
    "twitter = OAuth1Session(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_KEY_SECRET)\n",
    "print(twitter)\n",
    "\n",
    "#エンドポイント\n",
    "url_text = 'https://api.twitter.com/1.1/statuses/update.json'\n",
    "url_media = \"https://upload.twitter.com/1.1/media/upload.json\"\n",
    "\n",
    "\n",
    "\n",
    "# ここまでTwitter投稿の準備\n",
    "print(\"投稿準備完了\")\n",
    "\n",
    "\n",
    "#スニだんのページの指定\n",
    "URL = 'https://snkrdunk.com'\n",
    "# リクエストヘッダの指定\n",
    "headers = {\"User-Agent\": \"hoge\"}\n",
    "response = requests.get(URL,  headers=headers)\n",
    "r_text=response.text\n",
    "soup = BeautifulSoup(r_text, 'html.parser')\n",
    "\n",
    "\n",
    "print('スニダンのページの取得完了')\n",
    "\n",
    "\n",
    "\n",
    "# 記事の取得 \n",
    "soup_article=soup.find_all(\"article\",attrs={\"class\",\"article-list\"})[4]      \n",
    "soup_text=soup_article.find_all(\"h3\")[0].find(\"a\").text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"定価/\",\"\")\n",
    "# 画像の取得\n",
    "soup_img=soup_article.find_all(\"img\")[0]['src']\n",
    "# 詳細ページのリンクを取得\n",
    "soup_url=\"https://snkrdunk.com/\"+soup_article.find(\"a\")['href']\n",
    "# 画像の処理\n",
    "response = requests.get(soup_img)\n",
    "image = response.content\n",
    "files = {\"media\" : image}\n",
    "req_media = twitter.post(url_media, files = files)\n",
    "media_id = json.loads(req_media.text)['media_id']\n",
    "print('画像の取得完了')\n",
    "\n",
    "\n",
    "print(\"ここから詳細ページ\")\n",
    "try:\n",
    "    URL = soup_url\n",
    "    # リクエストヘッダの指定\n",
    "    headers = {\"User-Agent\": \"hoge\"}\n",
    "    response = requests.get(URL,  headers=headers)\n",
    "    r_text=response.text\n",
    "    soup = BeautifulSoup(r_text, 'html.parser')\n",
    "\n",
    "    if soup_text[:5]==\"【リーク】\":\n",
    "        soup_h1=soup.find_all(\"h1\",attrs={\"class\",\"page-title\"})[0].text.replace(\"抽選/定価/販売店舗まとめ\",\"\").replace(\"【リーク】\",\"\")\n",
    "        soup_text=soup.find_all(\"div\",attrs={\"class\",\"article-content\"})[0].text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "        pos1=soup_text.find(\"について\")\n",
    "        text1=soup_text[pos1+4:]\n",
    "        pos2=text1.find(\"発売予定！\")\n",
    "        if pos2<10: \n",
    "            pos3=text1.find(\"発売！\")\n",
    "            if pos3<10:\n",
    "                pos4=text1.find(\"復刻予定！\")\n",
    "                if pos4<10:\n",
    "                    pos5=text1.find(\"リリース！\")\n",
    "                    soup_cap=text1[:pos5+5]\n",
    "                    params = {'status':\"リーク情報!!!\\n\\n{}\\n\\n{}\\n\\n情報が入り次第更新!!!\".format(soup_h1,soup_cap),'media_ids':[media_id]}\n",
    "                else:\n",
    "                    soup_cap=text1[:pos4+5]\n",
    "                    params = {'status':\"リーク情報!!!\\n\\n{}\\n\\n{}\\n\\n情報が入り次第更新!!!\".format(soup_h1,soup_cap),'media_ids':[media_id]}\n",
    "            else:\n",
    "                soup_cap=text1[:pos3+3]\n",
    "                params = {'status':\"リーク情報!!!\\n\\n{}\\n\\n{}\\n\\n情報が入り次第更新!!!\".format(soup_h1,soup_cap),'media_ids':[media_id]}\n",
    "        else:\n",
    "            soup_cap=text1[:pos2+5]\n",
    "            params = {'status':\"リーク情報!!!\\n\\n{}\\n\\n{}\\n\\n情報が入り次第更新!!!\".format(soup_h1,soup_cap),'media_ids':[media_id]}\n",
    "        wb = openpyxl.load_workbook('/Users/kuramochiosuke/Desktop/スニーカー記事/sneaker.xlsx')\n",
    "        ws = wb[\"Sheet1\"]\n",
    "        for i in range(wb['Sheet1'].max_row):\n",
    "            if ws.cell(row=i+1,column=1).value==params[\"status\"]:\n",
    "                print(\"投稿済みです\")\n",
    "                break    \n",
    "            elif i==wb['Sheet1'].max_row-1:  \n",
    "                ws.cell(row=wb['Sheet1'].max_row+1,column=1).value = params[\"status\"]\n",
    "                wb.save('/Users/kuramochiosuke/Desktop/スニーカー記事/sneaker.xlsx')\n",
    "                print(\"保存しました\")\n",
    "                twitter.post(url_text, params = params)\n",
    "                print(\"投稿しました\")  \n",
    "        print(\"\")\n",
    "    elif soup_text[:9]==\"【販売リンクあり】\":\n",
    "        a_count=len(soup.find_all(\"div\",attrs={\"class\",\"sneaker-release-shop-box pre-release\"})[0].find_all(\"a\"))\n",
    "        for i in range(a_count):\n",
    "            soup_block=soup.find_all(\"div\",attrs={\"class\",\"sneaker-release-shop-box\"})[0].find_all(\"a\")[i]\n",
    "            soup_link=soup_block['href']\n",
    "            soup_app_name=soup_block.find_all(\"div\",attrs={\"class\",\"left-box\"})[0].text\n",
    "            soup_data=soup_block.find_all(\"div\",attrs={\"class\",\"shop-right-box\"})[0].text\n",
    "            params = {'status': \"{}\\n\\n{}  {}\\n{}\\n\".format(soup_text,soup_app_name,soup_data,soup_link),'media_ids':[media_id]}\n",
    "            wb = openpyxl.load_workbook('/Users/kuramochiosuke/Desktop/スニーカー記事/sneaker.xlsx')\n",
    "            ws = wb[\"Sheet1\"]\n",
    "            for i in range(wb['Sheet1'].max_row):\n",
    "                if ws.cell(row=i+1,column=1).value==params[\"status\"]:\n",
    "                    print(\"投稿済みです\")\n",
    "                    break    \n",
    "                elif i==wb['Sheet1'].max_row-1:   \n",
    "                    ws.cell(row=wb['Sheet1'].max_row+1,column=1).value = params[\"status\"]\n",
    "                    wb.save('/Users/kuramochiosuke/Desktop/スニーカー記事/sneaker.xlsx')\n",
    "                    print(\"保存しました\")\n",
    "                    twitter.post(url_text, params = params)\n",
    "                    print(\"投稿しました\")  \n",
    "            print(\"\")\n",
    "    else:\n",
    "        print(\"除外\")\n",
    "        print(\"\")\n",
    "except IndexError:\n",
    "    print(\"INDEX エラーです\")       \n",
    "    print(\"\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"NOT FILE エラーです\")       \n",
    "    print(\"\")\n",
    "\n",
    "except KeyError:\n",
    "    print(\"KeyError エラーです\")       \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86373cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ee031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
